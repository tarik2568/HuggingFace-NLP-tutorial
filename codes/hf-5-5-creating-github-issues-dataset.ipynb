{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install requests","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:33:56.729899Z","iopub.execute_input":"2023-08-04T14:33:56.730275Z","iopub.status.idle":"2023-08-04T14:34:05.960001Z","shell.execute_reply.started":"2023-08-04T14:33:56.730247Z","shell.execute_reply":"2023-08-04T14:34:05.959069Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2023.5.7)\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\n\nurl = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\nresponse = requests.get(url)\nresponse","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:34:05.961755Z","iopub.execute_input":"2023-08-04T14:34:05.962123Z","iopub.status.idle":"2023-08-04T14:34:06.138585Z","shell.execute_reply.started":"2023-08-04T14:34:05.962094Z","shell.execute_reply":"2023-08-04T14:34:06.137929Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<Response [200]>"},"metadata":{}}]},{"cell_type":"code","source":"response.json()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:34:06.139491Z","iopub.execute_input":"2023-08-04T14:34:06.140518Z","iopub.status.idle":"2023-08-04T14:34:06.149011Z","shell.execute_reply.started":"2023-08-04T14:34:06.140431Z","shell.execute_reply":"2023-08-04T14:34:06.147993Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6120',\n  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6120/labels{/name}',\n  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6120/comments',\n  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6120/events',\n  'html_url': 'https://github.com/huggingface/datasets/issues/6120',\n  'id': 1836026938,\n  'node_id': 'I_kwDODunzps5tb4w6',\n  'number': 6120,\n  'title': 'Lookahead streaming support?',\n  'user': {'login': 'PicoCreator',\n   'id': 17175484,\n   'node_id': 'MDQ6VXNlcjE3MTc1NDg0',\n   'avatar_url': 'https://avatars.githubusercontent.com/u/17175484?v=4',\n   'gravatar_id': '',\n   'url': 'https://api.github.com/users/PicoCreator',\n   'html_url': 'https://github.com/PicoCreator',\n   'followers_url': 'https://api.github.com/users/PicoCreator/followers',\n   'following_url': 'https://api.github.com/users/PicoCreator/following{/other_user}',\n   'gists_url': 'https://api.github.com/users/PicoCreator/gists{/gist_id}',\n   'starred_url': 'https://api.github.com/users/PicoCreator/starred{/owner}{/repo}',\n   'subscriptions_url': 'https://api.github.com/users/PicoCreator/subscriptions',\n   'organizations_url': 'https://api.github.com/users/PicoCreator/orgs',\n   'repos_url': 'https://api.github.com/users/PicoCreator/repos',\n   'events_url': 'https://api.github.com/users/PicoCreator/events{/privacy}',\n   'received_events_url': 'https://api.github.com/users/PicoCreator/received_events',\n   'type': 'User',\n   'site_admin': False},\n  'labels': [{'id': 1935892871,\n    'node_id': 'MDU6TGFiZWwxOTM1ODkyODcx',\n    'url': 'https://api.github.com/repos/huggingface/datasets/labels/enhancement',\n    'name': 'enhancement',\n    'color': 'a2eeef',\n    'default': True,\n    'description': 'New feature or request'}],\n  'state': 'open',\n  'locked': False,\n  'assignee': None,\n  'assignees': [],\n  'milestone': None,\n  'comments': 0,\n  'created_at': '2023-08-04T04:01:52Z',\n  'updated_at': '2023-08-04T04:02:04Z',\n  'closed_at': None,\n  'author_association': 'NONE',\n  'active_lock_reason': None,\n  'body': '### Feature request\\r\\n\\r\\nFrom what I understand, streaming dataset currently pulls the data, and process the data as it is requested.\\r\\nThis can introduce significant latency delays when data is loaded into the training process, needing to wait for each segment.\\r\\n\\r\\nWhile the delays might be dataset specific (or even mapping instruction/tokenizer specific)\\r\\n\\r\\nIs it possible to introduce a `streaming_lookahead` parameter, which is used for predictable workloads (even shuffled dataset with fixed seed). As we can predict in advance what the next few datasamples will be. And fetch them while the current set is being trained.\\r\\n\\r\\nWith enough CPU & bandwidth to keep up with the training process, and a sufficiently large lookahead, this will reduce the various latency involved while waiting for the dataset to be ready between batches.\\r\\n\\r\\n### Motivation\\r\\n\\r\\nFaster streaming performance, while training over extra large TB sized datasets\\r\\n\\r\\n### Your contribution\\r\\n\\r\\nI currently use HF dataset, with pytorch lightning trainer for RWKV project, and would be able to help test this feature if supported.',\n  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6120/reactions',\n   'total_count': 0,\n   '+1': 0,\n   '-1': 0,\n   'laugh': 0,\n   'hooray': 0,\n   'confused': 0,\n   'heart': 0,\n   'rocket': 0,\n   'eyes': 0},\n  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6120/timeline',\n  'performed_via_github_app': None,\n  'state_reason': None}]"},"metadata":{}}]},{"cell_type":"code","source":"GITHUB_TOKEN = \"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\"  # Copy your GitHub token here\nheaders = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:34:06.151660Z","iopub.execute_input":"2023-08-04T14:34:06.152765Z","iopub.status.idle":"2023-08-04T14:34:06.162501Z","shell.execute_reply.started":"2023-08-04T14:34:06.152735Z","shell.execute_reply":"2023-08-04T14:34:06.161061Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import time\nimport math\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n\ndef fetch_issues(\n    owner=\"huggingface\",\n    repo=\"datasets\",\n    num_issues=10_000,\n    rate_limit=5_000,\n    issues_path=Path(\".\"),\n):\n    if not issues_path.is_dir():\n        issues_path.mkdir(exist_ok=True)\n\n    batch = []\n    all_issues = []\n    per_page = 100  # Number of issues to return per page\n    num_pages = math.ceil(num_issues / per_page)\n    base_url = \"https://api.github.com/repos\"\n\n    for page in tqdm(range(num_pages)):\n        # Query with state=all to get both open and closed issues\n        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n        batch.extend(issues.json())\n\n        if len(batch) > rate_limit and len(all_issues) < num_issues:\n            all_issues.extend(batch)\n            batch = []  # Flush batch for next time period\n            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n            time.sleep(60 * 60 + 1)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:34:06.164717Z","iopub.execute_input":"2023-08-04T14:34:06.165434Z","iopub.status.idle":"2023-08-04T14:34:06.178042Z","shell.execute_reply.started":"2023-08-04T14:34:06.165396Z","shell.execute_reply":"2023-08-04T14:34:06.176394Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"fetch_issues()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T14:34:06.179989Z","iopub.execute_input":"2023-08-04T14:34:06.180399Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d328af06aeba474bad76bc681641098b"}},"metadata":{}},{"name":"stdout","text":"Reached GitHub rate limit. Sleeping for one hour ...\n","output_type":"stream"}]},{"cell_type":"code","source":"all_issues.extend(batch)\n    df = pd.DataFrame.from_records()\n    \n    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n    print(\n        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nissues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"test\")\nissues_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = issues_dataset.shuffle(seed=666).select(range(3))\n\n# Print out the URL and pull request entries\nfor url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n    print(f\">> URL: {url}\")\n    print(f\">> Pull request: {pr}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"issues_dataset = issues_dataset.map(\n    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"issue_number = 2792\nurl = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\nresponse = requests.get(url, headers=headers)\nresponse.json()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_comments(issue_number):\n    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n    response = requests.get(url, headers=headers)\n    return [r[\"body\"] for r in response.json()]\n\n\n# Test our function works as expected\nget_comments(2792)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Depending on your internet connection, this can take a few minutes...\nissues_with_comments_dataset = issues_dataset.map(\n    lambda x: {\"comments\": get_comments(x[\"number\"])}\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}