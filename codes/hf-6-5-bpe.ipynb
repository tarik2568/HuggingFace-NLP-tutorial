{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:29:52.310351Z","iopub.execute_input":"2023-08-07T19:29:52.310907Z","iopub.status.idle":"2023-08-07T19:29:54.754791Z","shell.execute_reply.started":"2023-08-07T19:29:52.310865Z","shell.execute_reply":"2023-08-07T19:29:54.753413Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"Héllò hôw are ü?\"\nText_normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\nprint(Text_normalized)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:29:54.757679Z","iopub.execute_input":"2023-08-07T19:29:54.758407Z","iopub.status.idle":"2023-08-07T19:29:55.913403Z","shell.execute_reply.started":"2023-08-07T19:29:54.758365Z","shell.execute_reply":"2023-08-07T19:29:55.911692Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aebed71828c94e49896977ced80b897e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d3d9bfce004fef80bb33742a923716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63d9ce34a21477e95e902eedb996f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3a52ed89d5456f996244465624c10e"}},"metadata":{}},{"name":"stdout","text":"hello how are u?\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"albert-large-v2\")\nprint(tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:29:55.915331Z","iopub.execute_input":"2023-08-07T19:29:55.915788Z","iopub.status.idle":"2023-08-07T19:29:57.375346Z","shell.execute_reply.started":"2023-08-07T19:29:55.915752Z","shell.execute_reply":"2023-08-07T19:29:57.374245Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9e91310c784c249373f007767f99be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2d76bb7d93d43ed82bfe34b5834dac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9327d196b9174d9f9106c32bbb2c5287"}},"metadata":{}},{"name":"stdout","text":"['[CLS]', '▁hello', '▁how', '▁are', '▁u', '?', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"Héllò hôw are ü?\"\nText_normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text)\nprint(Text_normalized)\nprint(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:29:57.377542Z","iopub.execute_input":"2023-08-07T19:29:57.378207Z","iopub.status.idle":"2023-08-07T19:29:57.529510Z","shell.execute_reply.started":"2023-08-07T19:29:57.378137Z","shell.execute_reply":"2023-08-07T19:29:57.527932Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"hello how are u?\n[('Héllò', (0, 5)), ('hôw', (6, 9)), ('are', (10, 13)), ('ü', (14, 15)), ('?', (15, 16))]\n","output_type":"stream"}]},{"cell_type":"code","source":"corpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:29:59.073870Z","iopub.execute_input":"2023-08-07T19:29:59.074364Z","iopub.status.idle":"2023-08-07T19:29:59.079804Z","shell.execute_reply.started":"2023-08-07T19:29:59.074324Z","shell.execute_reply":"2023-08-07T19:29:59.078738Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:30:05.388994Z","iopub.execute_input":"2023-08-07T19:30:05.389666Z","iopub.status.idle":"2023-08-07T19:30:06.785791Z","shell.execute_reply.started":"2023-08-07T19:30:05.389628Z","shell.execute_reply":"2023-08-07T19:30:06.784549Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da2dc20a41cf44ebab141b8bcddfc1ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39cb0aab23b4ec685044a031e7d9711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ce46d37d624e819414f6d81da65952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db9d822120504ae284e2930a7c1f80e5"}},"metadata":{}}]},{"cell_type":"code","source":"from collections import defaultdict\n\nword_freqs = defaultdict(int)\n\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    for word in new_words:\n        word_freqs[word] += 1\n\nprint(word_freqs)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:30:17.428363Z","iopub.execute_input":"2023-08-07T19:30:17.429587Z","iopub.status.idle":"2023-08-07T19:30:17.441203Z","shell.execute_reply.started":"2023-08-07T19:30:17.429535Z","shell.execute_reply":"2023-08-07T19:30:17.439970Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n","output_type":"stream"}]},{"cell_type":"code","source":"alphabet = []\n\nfor word in word_freqs.keys():\n    for letter in word:\n        if letter not in alphabet:\n            alphabet.append(letter)\nalphabet.sort()\n\nprint(alphabet)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:33:28.242471Z","iopub.execute_input":"2023-08-07T19:33:28.242998Z","iopub.status.idle":"2023-08-07T19:33:28.250899Z","shell.execute_reply.started":"2023-08-07T19:33:28.242962Z","shell.execute_reply":"2023-08-07T19:33:28.249219Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab = [\"<|endoftext|>\"] + alphabet.copy()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:34:07.658137Z","iopub.execute_input":"2023-08-07T19:34:07.658628Z","iopub.status.idle":"2023-08-07T19:34:07.665792Z","shell.execute_reply.started":"2023-08-07T19:34:07.658592Z","shell.execute_reply":"2023-08-07T19:34:07.664355Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"splits = {word: [c for c in word] for word in word_freqs.keys()}","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:34:46.771947Z","iopub.execute_input":"2023-08-07T19:34:46.772471Z","iopub.status.idle":"2023-08-07T19:34:46.779666Z","shell.execute_reply.started":"2023-08-07T19:34:46.772436Z","shell.execute_reply":"2023-08-07T19:34:46.778177Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"splits","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:36:27.005950Z","iopub.execute_input":"2023-08-07T19:36:27.006386Z","iopub.status.idle":"2023-08-07T19:36:27.020351Z","shell.execute_reply.started":"2023-08-07T19:36:27.006354Z","shell.execute_reply":"2023-08-07T19:36:27.018975Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'This': ['T', 'h', 'i', 's'],\n 'Ġis': ['Ġ', 'i', 's'],\n 'Ġthe': ['Ġ', 't', 'h', 'e'],\n 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'],\n 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'],\n '.': ['.'],\n 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'],\n 'Ġtokenization': ['Ġ',\n  't',\n  'o',\n  'k',\n  'e',\n  'n',\n  'i',\n  'z',\n  'a',\n  't',\n  'i',\n  'o',\n  'n'],\n 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'],\n 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n ',': [','],\n 'Ġyou': ['Ġ', 'y', 'o', 'u'],\n 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'],\n 'Ġbe': ['Ġ', 'b', 'e'],\n 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'],\n 'Ġto': ['Ġ', 't', 'o'],\n 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n 'Ġhow': ['Ġ', 'h', 'o', 'w'],\n 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'],\n 'Ġare': ['Ġ', 'a', 'r', 'e'],\n 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n 'Ġand': ['Ġ', 'a', 'n', 'd'],\n 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}"},"metadata":{}}]},{"cell_type":"code","source":"def compute_pair_freqs(splits):\n    pair_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i + 1])\n            pair_freqs[pair] += freq\n    return pair_freqs","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:37:08.352630Z","iopub.execute_input":"2023-08-07T19:37:08.353122Z","iopub.status.idle":"2023-08-07T19:37:08.362184Z","shell.execute_reply.started":"2023-08-07T19:37:08.353089Z","shell.execute_reply":"2023-08-07T19:37:08.360369Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pair_freqs = compute_pair_freqs(splits)\n\nfor i, key in enumerate(pair_freqs.keys()):\n    print(f\"{key}: {pair_freqs[key]}\")\n   ","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:38:54.150742Z","iopub.execute_input":"2023-08-07T19:38:54.151192Z","iopub.status.idle":"2023-08-07T19:38:54.160245Z","shell.execute_reply.started":"2023-08-07T19:38:54.151133Z","shell.execute_reply":"2023-08-07T19:38:54.158954Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"('T', 'h'): 3\n('h', 'i'): 3\n('i', 's'): 5\n('Ġ', 'i'): 2\n('Ġ', 't'): 7\n('t', 'h'): 3\n('h', 'e'): 2\n('Ġ', 'H'): 1\n('H', 'u'): 1\n('u', 'g'): 1\n('g', 'g'): 1\n('g', 'i'): 1\n('i', 'n'): 2\n('n', 'g'): 1\n('Ġ', 'F'): 1\n('F', 'a'): 1\n('a', 'c'): 1\n('c', 'e'): 1\n('Ġ', 'C'): 1\n('C', 'o'): 1\n('o', 'u'): 3\n('u', 'r'): 1\n('r', 's'): 2\n('s', 'e'): 3\n('Ġ', 'c'): 1\n('c', 'h'): 1\n('h', 'a'): 1\n('a', 'p'): 1\n('p', 't'): 1\n('t', 'e'): 2\n('e', 'r'): 5\n('Ġ', 'a'): 5\n('a', 'b'): 2\n('b', 'o'): 1\n('u', 't'): 1\n('t', 'o'): 4\n('o', 'k'): 3\n('k', 'e'): 3\n('e', 'n'): 4\n('n', 'i'): 2\n('i', 'z'): 2\n('z', 'a'): 1\n('a', 't'): 2\n('t', 'i'): 2\n('i', 'o'): 2\n('o', 'n'): 2\n('Ġ', 's'): 3\n('e', 'c'): 1\n('c', 't'): 1\n('s', 'h'): 1\n('h', 'o'): 2\n('o', 'w'): 2\n('w', 's'): 1\n('e', 'v'): 1\n('v', 'e'): 1\n('r', 'a'): 3\n('a', 'l'): 2\n('z', 'e'): 1\n('l', 'g'): 1\n('g', 'o'): 1\n('o', 'r'): 1\n('r', 'i'): 1\n('i', 't'): 1\n('h', 'm'): 1\n('m', 's'): 1\n('H', 'o'): 1\n('o', 'p'): 1\n('p', 'e'): 1\n('e', 'f'): 1\n('f', 'u'): 1\n('u', 'l'): 1\n('l', 'l'): 2\n('l', 'y'): 1\n('Ġ', 'y'): 1\n('y', 'o'): 1\n('Ġ', 'w'): 1\n('w', 'i'): 1\n('i', 'l'): 1\n('Ġ', 'b'): 1\n('b', 'e'): 1\n('b', 'l'): 1\n('l', 'e'): 1\n('Ġ', 'u'): 1\n('u', 'n'): 1\n('n', 'd'): 3\n('d', 'e'): 1\n('s', 't'): 1\n('t', 'a'): 1\n('a', 'n'): 2\n('Ġ', 'h'): 1\n('e', 'y'): 1\n('a', 'r'): 1\n('r', 'e'): 1\n('t', 'r'): 1\n('a', 'i'): 1\n('n', 'e'): 2\n('e', 'd'): 1\n('Ġ', 'g'): 1\n('g', 'e'): 1\n('n', 's'): 1\n","output_type":"stream"}]},{"cell_type":"code","source":"best_pair = \"\"\nmax_freq = None\n\nfor pair, freq in pair_freqs.items():\n    if max_freq is None or max_freq < freq:\n        best_pair = pair\n        max_freq = freq\n\nprint(best_pair, max_freq)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:39:15.163430Z","iopub.execute_input":"2023-08-07T19:39:15.163895Z","iopub.status.idle":"2023-08-07T19:39:15.171823Z","shell.execute_reply.started":"2023-08-07T19:39:15.163859Z","shell.execute_reply":"2023-08-07T19:39:15.170291Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"('Ġ', 't') 7\n","output_type":"stream"}]},{"cell_type":"code","source":"merges = {(\"Ġ\", \"t\"): \"Ġt\"}\nvocab.append(\"Ġt\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:39:23.306141Z","iopub.execute_input":"2023-08-07T19:39:23.306584Z","iopub.status.idle":"2023-08-07T19:39:23.313099Z","shell.execute_reply.started":"2023-08-07T19:39:23.306549Z","shell.execute_reply":"2023-08-07T19:39:23.311674Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue\n\n        i = 0\n        while i < len(split) - 1:\n            if split[i] == a and split[i + 1] == b:\n                split = split[:i] + [a + b] + split[i + 2 :]\n            else:\n                i += 1\n        splits[word] = split\n    return splits","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:39:39.210849Z","iopub.execute_input":"2023-08-07T19:39:39.211368Z","iopub.status.idle":"2023-08-07T19:39:39.220989Z","shell.execute_reply.started":"2023-08-07T19:39:39.211330Z","shell.execute_reply":"2023-08-07T19:39:39.219246Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"splits = merge_pair(\"Ġ\", \"t\", splits)\nprint(splits[\"Ġtrained\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:39:47.738446Z","iopub.execute_input":"2023-08-07T19:39:47.739582Z","iopub.status.idle":"2023-08-07T19:39:47.746441Z","shell.execute_reply.started":"2023-08-07T19:39:47.739542Z","shell.execute_reply":"2023-08-07T19:39:47.744986Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = 50\n\nwhile len(vocab) < vocab_size:\n    pair_freqs = compute_pair_freqs(splits)\n    best_pair = \"\"\n    max_freq = None\n    for pair, freq in pair_freqs.items():\n        if max_freq is None or max_freq < freq:\n            best_pair = pair\n            max_freq = freq\n    splits = merge_pair(*best_pair, splits)\n    merges[best_pair] = best_pair[0] + best_pair[1]\n    vocab.append(best_pair[0] + best_pair[1])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:40:08.684248Z","iopub.execute_input":"2023-08-07T19:40:08.684665Z","iopub.status.idle":"2023-08-07T19:40:08.697322Z","shell.execute_reply.started":"2023-08-07T19:40:08.684633Z","shell.execute_reply":"2023-08-07T19:40:08.695977Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(merges)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:40:20.524344Z","iopub.execute_input":"2023-08-07T19:40:20.525804Z","iopub.status.idle":"2023-08-07T19:40:20.531197Z","shell.execute_reply.started":"2023-08-07T19:40:20.525759Z","shell.execute_reply":"2023-08-07T19:40:20.530017Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:40:28.292699Z","iopub.execute_input":"2023-08-07T19:40:28.293193Z","iopub.status.idle":"2023-08-07T19:40:28.300127Z","shell.execute_reply.started":"2023-08-07T19:40:28.293122Z","shell.execute_reply":"2023-08-07T19:40:28.298779Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(text):\n    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n    splits = [[l for l in word] for word in pre_tokenized_text]\n    for pair, merge in merges.items():\n        for idx, split in enumerate(splits):\n            i = 0\n            while i < len(split) - 1:\n                if split[i] == pair[0] and split[i + 1] == pair[1]:\n                    split = split[:i] + [merge] + split[i + 2 :]\n                else:\n                    i += 1\n            splits[idx] = split\n\n    return sum(splits, [])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:40:41.211926Z","iopub.execute_input":"2023-08-07T19:40:41.212434Z","iopub.status.idle":"2023-08-07T19:40:41.222618Z","shell.execute_reply.started":"2023-08-07T19:40:41.212398Z","shell.execute_reply":"2023-08-07T19:40:41.221667Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenize(\"This is not a token.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T19:40:48.015069Z","iopub.execute_input":"2023-08-07T19:40:48.015591Z","iopub.status.idle":"2023-08-07T19:40:48.025611Z","shell.execute_reply.started":"2023-08-07T19:40:48.015547Z","shell.execute_reply":"2023-08-07T19:40:48.024175Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"},"metadata":{}}]}]}