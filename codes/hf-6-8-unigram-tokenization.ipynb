{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"corpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:21.626823Z","iopub.execute_input":"2023-08-08T15:26:21.627309Z","iopub.status.idle":"2023-08-08T15:26:21.634108Z","shell.execute_reply.started":"2023-08-08T15:26:21.627275Z","shell.execute_reply":"2023-08-08T15:26:21.632650Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:21.637772Z","iopub.execute_input":"2023-08-08T15:26:21.638460Z","iopub.status.idle":"2023-08-08T15:26:22.255158Z","shell.execute_reply.started":"2023-08-08T15:26:21.638425Z","shell.execute_reply":"2023-08-08T15:26:22.253761Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nword_freqs = defaultdict(int)\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    for word in new_words:\n        word_freqs[word] += 1\n\nword_freqs","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.258029Z","iopub.execute_input":"2023-08-08T15:26:22.258781Z","iopub.status.idle":"2023-08-08T15:26:22.271870Z","shell.execute_reply.started":"2023-08-08T15:26:22.258722Z","shell.execute_reply":"2023-08-08T15:26:22.270358Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"defaultdict(int,\n            {'▁This': 3,\n             '▁is': 2,\n             '▁the': 1,\n             '▁Hugging': 1,\n             '▁Face': 1,\n             '▁Course.': 1,\n             '▁chapter': 1,\n             '▁about': 1,\n             '▁tokenization.': 1,\n             '▁section': 1,\n             '▁shows': 1,\n             '▁several': 1,\n             '▁tokenizer': 1,\n             '▁algorithms.': 1,\n             '▁Hopefully,': 1,\n             '▁you': 1,\n             '▁will': 1,\n             '▁be': 1,\n             '▁able': 1,\n             '▁to': 1,\n             '▁understand': 1,\n             '▁how': 1,\n             '▁they': 1,\n             '▁are': 1,\n             '▁trained': 1,\n             '▁and': 1,\n             '▁generate': 1,\n             '▁tokens.': 1})"},"metadata":{}}]},{"cell_type":"code","source":"char_freqs = defaultdict(int)\nsubwords_freqs = defaultdict(int)\nfor word, freq in word_freqs.items():\n    for i in range(len(word)):\n        char_freqs[word[i]] += freq\n        # Loop through the subwords of length at least 2\n        for j in range(i + 2, len(word) + 1):\n            subwords_freqs[word[i:j]] += freq\n\n# Sort subwords by frequency\nsorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\nsorted_subwords[:10]","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.273551Z","iopub.execute_input":"2023-08-08T15:26:22.274426Z","iopub.status.idle":"2023-08-08T15:26:22.290864Z","shell.execute_reply.started":"2023-08-08T15:26:22.274379Z","shell.execute_reply":"2023-08-08T15:26:22.289354Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[('▁t', 7),\n ('is', 5),\n ('er', 5),\n ('▁a', 5),\n ('▁to', 4),\n ('to', 4),\n ('en', 4),\n ('▁T', 3),\n ('▁Th', 3),\n ('▁Thi', 3)]"},"metadata":{}}]},{"cell_type":"code","source":"token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\ntoken_freqs = {token: freq for token, freq in token_freqs}","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.294013Z","iopub.execute_input":"2023-08-08T15:26:22.294452Z","iopub.status.idle":"2023-08-08T15:26:22.308201Z","shell.execute_reply.started":"2023-08-08T15:26:22.294419Z","shell.execute_reply":"2023-08-08T15:26:22.306650Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from math import log\n\ntotal_sum = sum([freq for token, freq in token_freqs.items()])\nmodel = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.309926Z","iopub.execute_input":"2023-08-08T15:26:22.310321Z","iopub.status.idle":"2023-08-08T15:26:22.321910Z","shell.execute_reply.started":"2023-08-08T15:26:22.310282Z","shell.execute_reply":"2023-08-08T15:26:22.320866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def encode_word(word, model):\n    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n        {\"start\": None, \"score\": None} for _ in range(len(word))\n    ]\n    for start_idx in range(len(word)):\n        # This should be properly filled by the previous steps of the loop\n        best_score_at_start = best_segmentations[start_idx][\"score\"]\n        for end_idx in range(start_idx + 1, len(word) + 1):\n            token = word[start_idx:end_idx]\n            if token in model and best_score_at_start is not None:\n                score = model[token] + best_score_at_start\n                # If we have found a better segmentation ending at end_idx, we update\n                if (\n                    best_segmentations[end_idx][\"score\"] is None\n                    or best_segmentations[end_idx][\"score\"] > score\n                ):\n                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n\n    segmentation = best_segmentations[-1]\n    if segmentation[\"score\"] is None:\n        # We did not find a tokenization of the word -> unknown\n        return [\"<unk>\"], None\n\n    score = segmentation[\"score\"]\n    start = segmentation[\"start\"]\n    end = len(word)\n    tokens = []\n    while start != 0:\n        tokens.insert(0, word[start:end])\n        next_start = best_segmentations[start][\"start\"]\n        end = start\n        start = next_start\n    tokens.insert(0, word[start:end])\n    return tokens, score","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.323696Z","iopub.execute_input":"2023-08-08T15:26:22.324113Z","iopub.status.idle":"2023-08-08T15:26:22.338846Z","shell.execute_reply.started":"2023-08-08T15:26:22.324079Z","shell.execute_reply":"2023-08-08T15:26:22.337846Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(encode_word(\"Hopefully\", model))\nprint(encode_word(\"This\", model))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.340522Z","iopub.execute_input":"2023-08-08T15:26:22.341208Z","iopub.status.idle":"2023-08-08T15:26:22.358361Z","shell.execute_reply.started":"2023-08-08T15:26:22.341171Z","shell.execute_reply":"2023-08-08T15:26:22.357129Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n(['This'], 6.288267030694535)\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_loss(model):\n    loss = 0\n    for word, freq in word_freqs.items():\n        _, word_loss = encode_word(word, model)\n        loss += freq * word_loss\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.359693Z","iopub.execute_input":"2023-08-08T15:26:22.360746Z","iopub.status.idle":"2023-08-08T15:26:22.368842Z","shell.execute_reply.started":"2023-08-08T15:26:22.360707Z","shell.execute_reply":"2023-08-08T15:26:22.367850Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"compute_loss(model)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.370526Z","iopub.execute_input":"2023-08-08T15:26:22.371694Z","iopub.status.idle":"2023-08-08T15:26:22.386972Z","shell.execute_reply.started":"2023-08-08T15:26:22.371649Z","shell.execute_reply":"2023-08-08T15:26:22.385589Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"413.10377642940875"},"metadata":{}}]},{"cell_type":"code","source":"import copy\n\n\ndef compute_scores(model):\n    scores = {}\n    model_loss = compute_loss(model)\n    for token, score in model.items():\n        # We always keep tokens of length 1\n        if len(token) == 1:\n            continue\n        model_without_token = copy.deepcopy(model)\n        _ = model_without_token.pop(token)\n        scores[token] = compute_loss(model_without_token) - model_loss\n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.390318Z","iopub.execute_input":"2023-08-08T15:26:22.390799Z","iopub.status.idle":"2023-08-08T15:26:22.400074Z","shell.execute_reply.started":"2023-08-08T15:26:22.390768Z","shell.execute_reply":"2023-08-08T15:26:22.399224Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"scores = compute_scores(model)\nprint(scores[\"ll\"])\nprint(scores[\"his\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.401123Z","iopub.execute_input":"2023-08-08T15:26:22.402155Z","iopub.status.idle":"2023-08-08T15:26:22.836484Z","shell.execute_reply.started":"2023-08-08T15:26:22.402089Z","shell.execute_reply":"2023-08-08T15:26:22.835568Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"6.376412403623874\n0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"percent_to_remove = 0.1\nwhile len(model) > 100:\n    scores = compute_scores(model)\n    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n    # Remove percent_to_remove tokens with the lowest scores.\n    for i in range(int(len(model) * percent_to_remove)):\n        _ = token_freqs.pop(sorted_scores[i][0])\n\n    total_sum = sum([freq for token, freq in token_freqs.items()])\n    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:22.838070Z","iopub.execute_input":"2023-08-08T15:26:22.839610Z","iopub.status.idle":"2023-08-08T15:26:25.077953Z","shell.execute_reply.started":"2023-08-08T15:26:22.839556Z","shell.execute_reply":"2023-08-08T15:26:25.076301Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def tokenize(text, model):\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in words_with_offsets]\n    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n    return sum(encoded_words, [])\n\n\ntokenize(\"This is the Hugging Face course.\", model)","metadata":{"execution":{"iopub.status.busy":"2023-08-08T15:26:25.079616Z","iopub.execute_input":"2023-08-08T15:26:25.080014Z","iopub.status.idle":"2023-08-08T15:26:25.091213Z","shell.execute_reply.started":"2023-08-08T15:26:25.079982Z","shell.execute_reply":"2023-08-08T15:26:25.089636Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['▁This',\n '▁is',\n '▁the',\n '▁Hugging',\n '▁Face',\n '▁',\n 'c',\n 'ou',\n 'r',\n 's',\n 'e',\n '.']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}